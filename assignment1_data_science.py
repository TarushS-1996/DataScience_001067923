# -*- coding: utf-8 -*-
"""Assignment1 Data Science.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCpu4cuZN9ph6FA0uX-e3ilimn4N_iK_

# Assignment 1 :

### Abstract:
Attrition is the departure of employees from the organization for any reason (voluntary or involuntary), including resignation, termination, death or retirement. In this dataset we are to explore a finctional data created by IBM to find a way to figure out the reasons for attrition or if any key features are related. In this assignment we will also answer the following questions regarding the assignment:
- What are the data types? (Only numeric and categorical)
- Are there missing values?

- What are the likely distributions of the numeric variables?

- Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)

- Which independent variables have missing data? How much? 

- Do the training and test sets have the same data?

- In the predictor variables independent of all the other predictor variables?

- Which predictor variables are the most important?

- Do the ranges of the predictor variables make sense?

- What are the distributions of the predictor variables?   

- Remove outliers and keep outliers (does if have an effect of the final predictive model)?

- Remove 1%, 5%, and 10% of your data randomly and impute the values back using at least 3 imputation methods. How well did the methods recover the missing values?  That is remove some data, check the % error on residuals for numeric data and check for bias and variance of the error.

### Importing the basic libraries required.
"""

!pip install shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tqdm as tqdm
import seaborn as sns

#data = pd.read_csv("dataset.csv")
import shap
data = pd.read_csv("https://raw.githubusercontent.com/TarushS-1996/DataScience_001067923/main/IBMHRAttritionDataset.csv")
print(data)

"""## **Question:** What are the data types? (Only numeric and categorical)
The data types are mostly numeric data except for business travel, Attrition, Department and Education field

## **Question:** Which independent variables have missing data? How much? **Question:** Are there missing values?
"""

data.isnull().sum()

"""**Answer:** Above we can see that all the fields have a certain value and do not require any imputation to fill the dataset.

### Data cleaning 
The dataset contains a column to check whether an employee is over 18 years of age. As it is a legal requirement it can be dropped. Similarly columns like HourlyRate, DailyRate EmployeeCount, Education, EducationField can be dropped as they don't directly corelate to attrition reason.
"""

#data.drop(['HourlyRate', 'DailyRate', 'EmployeeCount', 'Education', 'EducationField'], axis=1, inplace=True)
one_hot = {'Yes': 1, 'No': 0, 'Y':1, 'N':0, 'Travel_Rarely':2, 'Travel_Frequently':1, 'Non-Travel':3, 'Research & Development':1, 'Sales':2, 'Human Resources': 3}
data.Attrition = [one_hot[item] for item in data.Attrition]
data.Over18 = [one_hot[item] for item in data.Over18]
data.BusinessTravel = [one_hot[item] for item in data.BusinessTravel]
data.Department = [one_hot[item] for item in data.Department]
item_count = data.Department.value_counts()
print(item_count)

data_main = data[['Age', 'BusinessTravel', 'DistanceFromHome', 'EnvironmentSatisfaction', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel', 'TotalWorkingYears', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'Attrition']]
print(data_main.head())

"""## **Question:** How are the values distributed?
For this we can use the .describe() method.
"""

data_main.describe()

"""Above we get a general description of numeric data which mentions the mean, standard deviation, min value, max value etc. From this we can see that all values are positive.


## **Question:** Do the ranges of the predictor variables make sense?
**Answer:**From the above description we can see that the distribution of the values makes sense.

## **Question:**  What are the likely distributions of the numeric variables?
"""

from statsmodels.graphics.gofplots import qqplot
data_col = data_main[['Age', 'DistanceFromHome', 'EnvironmentSatisfaction', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel', 'TotalWorkingYears', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'Attrition']]
for c in data_col.columns[:]:
  plt.figure(figsize=(8,5))
  fig=qqplot(data_col[c],line='45',fit='True')
  plt.xticks(fontsize=13)
  plt.yticks(fontsize=13)
  plt.xlabel("Theoretical quantiles",fontsize=15)
  plt.ylabel("Sample quantiles",fontsize=15)
  plt.title("Q-Q plot of {}".format(c),fontsize=16)
  plt.grid(True)
  plt.show()

"""The distribution of the value are different for all the columns. The distribution for each of the plots are as following.

|Column name | Distributions|
|------------|:-------------|
| Age        | Continuous distribution|
|DistanceFromHome | Continuous distribution|
|EnvironementSatisfaction| Step distribution |
|JobSatisfaction| Step distribution|
|MonthlyIncome| Continuous distribution|
|MonthlyRate| Continuous distribution|
|NumOfCompaniesWorked| Step distribution|
|PercentOfSalaryHike| Step distribution|
|PerformanceRating | Step distribution|
|StockOptionLevel | Step distribution|
|TotalWorkingYears|Right-skew distribution|
|WorkLifeBalance| Step distribution|
|YearsAtCompany| Right-Skew distribution|
|YearsInCurrentRole|Right-skew|
|YearsSinceLastPromotion|Right-skew|
|Attrition| Step distribution|

# **Data normalization**
We have data and the respective features selected. We want to predict for attrition based on the features we have selected.
"""

plt.figure(figsize=(20, 7))
sns.boxplot(data=data_main)

"""We need to normalize the data for MonthlyIncome and JobSatisfaction. """

from sklearn import preprocessing
#data_main = data[['Age', 'BusinessTravel', 'DistanceFromHome', 'EnvironmentSatisfaction', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel', 'TotalWorkingYears', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'Attrition']]

dataScaled = pd.get_dummies(data_main)
min_max_scalar = preprocessing.MinMaxScaler()

xJS = dataScaled[['JobSatisfaction']]
xS = min_max_scalar.fit_transform(xJS)
dataScaled[['JobSatisfaction']] = pd.DataFrame(xS)

xMI = dataScaled[['MonthlyIncome']].values.astype(int)
xMIS = min_max_scalar.fit_transform(xMI)
dataScaled['MonthlyIncome'] = pd.DataFrame(xMIS)

xMR = dataScaled[['MonthlyRate']].values.astype(int)
xMRS = min_max_scalar.fit_transform(xMR)
dataScaled['MonthlyRate'] = pd.DataFrame(xMRS)

xA = dataScaled[['Age']].values.astype(int)
xAS = min_max_scalar.fit_transform(xA)
dataScaled['Age'] = pd.DataFrame(xAS)

#for (colName, colValue) in dataScaled.iteritems():
#    xOG = min_max_scalar.fit_transform(colValue)

"""xD = dataScaled[['DistanceFromHome']].values.astype(int)
xDS = min_max_scalar.fit_transform(xD)
dataScaled['DistanceFromHome'] = pd.DataFrame(xDS)



xPSH = dataScaled[['PercentSalaryHike']].values.astype(int)
xPSHS = min_max_scalar.fit_transform(xPSH)
dataScaled['PercentSalaryHike'] = pd.DataFrame(xPSHS)

xTWY = dataScaled[['TotalWorkingYears']].values.astype(int)
xTWYS = min_max_scalar.fit_transform(xTWY)
dataScaled['TotalWorkingYears'] = pd.DataFrame(xTWYS)"""

data_main = dataScaled

plt.figure(figsize=(50, 20))
sns.boxplot(data=data_main)

"""From the table we can see that TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion and NumOfCompaniesWorked have outliers. If we go back to the QQ plot of these variables we can see that 

|Column Name|Meaning|Distribution|
|-----------|:------|:-----------|
|TotalWorkingYears|values are clustered around the left tail of the distribution |Right-skew distribution|
|YearsAtCompany|values are clustered around the left tail of the distribution|Right-skew distribution|
|YearsInCurrentRole|values are clustered around the left tail of the distribution|Right-skew distribution|
|YearsSinceLastPromotion|values are clustered around the left tail of the distribution|Right-skew distribution|

### **Question:** What are the distributions of the predictor variables?

**Answer:** From above table we can the distribution of predictor variables.

## **Question:** Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)

# **Answer:**

# **Method 1:**
By using correlations we can see that certain values have a higher importance. To better visualize this we will use heatmap.
"""

data_main.corr()

plt.figure(figsize=(20,8))
sns.heatmap(data_main.corr(), annot=True, cmap='RdYlGn')

"""### **Question:** Which predictor variables are the most important?

From this we can relate that attrition is more dependent on Performance rating, Monthly rate, number of companies worked at, distance from home, work life balance.
### **Question:** In the predictor variables independent of all the other predictor variables?

**Answer:** From the above correlation matrix we can see that some of the variables are dependant on the other. They are:

|Value 1|Value 2|
|---|:---|
|MonthlyIncome|TotalWorkingYears|
|Age|TotalWorkingYears|
|YearsAtCompany|YearsInCurrentRole|

# **Method 2:**
By using model summary using ordinary least squared regression, we can see that higher T-statistic value gives more importance to the significance of the predictor variables.
"""

import statsmodels.api as sm

model = sm.OLS(data_main.Attrition, data_main).fit()
model.summary()

"""# **Method 3:**
We can also use model feature importance to get a better idea of the values that can be used to predict better variables. For this we will use model coefficients.

For this we will need to do the following:
1. Build a model
2. Fit the values of X and Y to the model
3. Use model.coef_ to get the values determining the important variables.

# Building model
"""

from sklearn.model_selection import train_test_split

x = data_main[['Age', 'BusinessTravel', 'DistanceFromHome', 'EnvironmentSatisfaction', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel', 'TotalWorkingYears', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion']]
#x = data_main[['BusinessTravel', 'DistanceFromHome', 'Department', 'JobSatisfaction', 'PercentSalaryHike', 'WorkLifeBalance', 'YearsAtCompany']]
y = data_main[['Attrition']]

print(x.head())

X_t, X_test, y_t, y_test = train_test_split(x, y, test_size=0.1, random_state=1)
x_train, x_val, y_train, y_val = train_test_split(X_t, y_t, test_size=0.15, random_state=1)

"""# **Question:** Do the training and test sets have the same data?


"""

check_df = pd.merge(
    X_t,
    X_test,
    on=['Age', 'BusinessTravel', 'DistanceFromHome', 'EnvironmentSatisfaction', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel', 'TotalWorkingYears', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion'],
    how="left",
    indicator="Exist",
)
check_df["Exist"] = np.where(check_df.Exist == "both", True, False)
print(
    check_df["Exist"].value_counts()
)

"""**Answer:** From the above code, we can see that the train and test sets are different. """

from sklearn.metrics import r2_score, mean_squared_error 
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn import datasets, linear_model
from sklearn.metrics import accuracy_score

scaler = preprocessing.StandardScaler().fit(x_train)
lregr = LogisticRegression(solver='lbfgs', max_iter=300)
lregr.fit(x_train, y_train.values.ravel())

print(lregr.coef_)
#ax = x_train.plot.bar(y = lregr.coef_)
pad = pd.DataFrame()
pad['ColumnNames'] = x_train.columns.values
pad['Coefficients'] = lregr.coef_[0]
pad.plot.bar()
print(pad)

"""From this we can figure out that the values we predicted to be important variables are same across the different methods. Now we can progress with predicting the attrition value. Now as we can see there are negative values. This basically states that the the values are negatively impacting the Y variable; meaning if these values are higher, chance of attrition becomes lower.

# **Question:** Remove outliers and keep outliers (does if have an effect of the final predictive model)?

For this we create two versions of train, test and validation set. One with outliers, one without outliers.

## Using Logistic regression to predict attrition with outliers:
"""

y_predict = lregr.predict(x_train)
print('Mean squared error is: {}'.format(mean_squared_error(y_train, y_predict)))
print('Coefficient of determination: %.2f'% r2_score(y_train, y_predict))
r2 = r2_score(y_train,y_predict)
print('R^2 score on tarining set =',r2)
score = accuracy_score(y_train,y_predict)
print("Accuracy score is: {}".format(score))

y_pred = lregr.predict(X_test)
print('Mean squared error is: {}'.format(mean_squared_error(y_test,y_pred)))
print('Coefficient of determination: %.2f'% r2_score(y_test, y_pred))
r2 = r2_score(y_test,y_pred)
print('R^2 score on tarining set =',r2)
score = accuracy_score(y_test,y_pred)
print("Accuracy score is: {}".format(score))

y_pred = lregr.predict(x_val)
print('Mean squared error is: {}'.format(mean_squared_error(y_val,y_pred)))
print('Coefficient of determination: %.2f'% r2_score(y_val, y_pred))
r2 = r2_score(y_val,y_pred)
print('R^2 score on tarining set =',r2)
score = accuracy_score(y_val,y_pred)
print("Accuracy score is: {}".format(score))

"""# **Using Logistic regression to predict attrition with outliers**
Now we remove the outliers from the dataset and repeat the logistic regression.
"""

from scipy import stats
def removeOutliers(df):
  data = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]
  return data

# Removing outliers from the datasets for training and testing
data_main_NoOutliers = removeOutliers(data_main)
x = data_main_NoOutliers[['Age', 'BusinessTravel', 'DistanceFromHome', 'EnvironmentSatisfaction', 'Department', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'StockOptionLevel', 'TotalWorkingYears', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion']]
#x = data_main[['BusinessTravel', 'DistanceFromHome', 'Department', 'JobSatisfaction', 'PercentSalaryHike', 'WorkLifeBalance', 'YearsAtCompany']]
y = data_main_NoOutliers[['Attrition']]
X_t, X_test, y_t, y_test = train_test_split(x, y, test_size=0.1, random_state=1)
x_train, x_val, y_train, y_val = train_test_split(X_t, y_t, test_size=0.15, random_state=1)
scaler = preprocessing.StandardScaler().fit(x_train)
lregr = LogisticRegression(solver='lbfgs', max_iter=300)
lregr.fit(x_train, y_train.values.ravel())

y_predict = lregr.predict(x_train)
print('Mean squared error is: {}'.format(mean_squared_error(y_train, y_predict)))
print('Coefficient of determination: %.2f'% r2_score(y_train, y_predict))
r2 = r2_score(y_train,y_predict)
print('R^2 score on tarining set =',r2)
score = accuracy_score(y_train,y_predict)
print("Accuracy score is: {}".format(score))

y_pred = lregr.predict(X_test)
print('Mean squared error is: {}'.format(mean_squared_error(y_test,y_pred)))
print('Coefficient of determination: %.2f'% r2_score(y_test, y_pred))
r2 = r2_score(y_test,y_pred)
print('R^2 score on tarining set =',r2)
score = accuracy_score(y_test,y_pred)
print("Accuracy score is: {}".format(score))

y_pred = lregr.predict(x_val)
print('Mean squared error is: {}'.format(mean_squared_error(y_val,y_pred)))
print('Coefficient of determination: %.2f'% r2_score(y_val, y_pred))
r2 = r2_score(y_val,y_pred)
print('R^2 score on tarining set =',r2)
score = accuracy_score(y_val,y_pred)
print("Accuracy score is: {}".format(score))

"""**Answer:** Yes the removal of outliers has some reduction in scores.

### **Question:** Remove 1%, 5%, and 10% of your data randomly and impute the values back using at least 3 imputation methods. How well did the methods recover the missing values? That is remove some data, check the % error on residuals for numeric data and check for bias and variance of the error.
# Imputing the data for 1%, 5% and 10%
Now we want to impute data for the column 'TotalWorkingYears'. If we return to correlations table, we can see that the TotalWorkingYears is more dependant on the values 'Age', 'MonthlyIncome', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion' and 'NumCompaniesWorked' and thus will be more important to imputing the values with lesser percent change to the orignal values.
"""

from sklearn.impute import KNNImputer
from sklearn.impute import SimpleImputer
data_mainImpute = data_main.copy(deep=True)
def removeRandomData(dataset, percent, col):
    dataset.loc[dataset.sample(frac = percent).index, col] = np.nan
#removeRandomData(data_mainImpute, 0.05, 'TotalWorkingYears')
#Imputation of 10% of the data
data_mainImpute['TotalWorkingYears_10'] = data_mainImpute['TotalWorkingYears']
data_mainImpute['TotalWorkingYears_05'] = data_mainImpute['TotalWorkingYears']
data_mainImpute['TotalWorkingYears_01'] = data_mainImpute['TotalWorkingYears']

removeRandomData(data_mainImpute, 0.10, 'TotalWorkingYears_10')
removeRandomData(data_mainImpute, 0.05, 'TotalWorkingYears_05')
removeRandomData(data_mainImpute, 0.01, 'TotalWorkingYears_01')
print(data_mainImpute.isnull().sum())

"""<h4>Making dataframes for different percent removed data</h4>"""

df_number1 = data_mainImpute[['Age', 'MonthlyIncome', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'NumCompaniesWorked', 'TotalWorkingYears_01']]
df_number5 = data_mainImpute[['Age', 'MonthlyIncome', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'NumCompaniesWorked', 'TotalWorkingYears_05']]
df_number10 = data_mainImpute[['Age', 'MonthlyIncome', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'NumCompaniesWorked', 'TotalWorkingYears_10']]
df_orignal = data_mainImpute['TotalWorkingYears']

### Specifying the imputer. ###
mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

### FOR the 1 % imputed values using Mean method ###
mean_imputer = mean_imputer.fit(df_number1)
results = mean_imputer.transform(df_number1.values)
imputed_number_df = pd.DataFrame(mean_imputer.transform(results), columns = df_number1.columns)
print(imputed_number_df.isnull().sum())
results.round()
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_01'] = imputed_number_df['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_01'], axis = 0)
percent_change_1 = (imputed_number_df['orignalVimputed_01'].sum() / imputed_number_df['orignalVimputed_01'].count())*100

mean_imputer = mean_imputer.fit(df_number5)
results = mean_imputer.transform(df_number5.values)
imputed_number_df = pd.DataFrame(mean_imputer.transform(results), columns = df_number5.columns)
results.round()
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_05'] = imputed_number_df['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_05'], axis = 0)
percent_change_5 = (imputed_number_df['orignalVimputed_05'].sum() / imputed_number_df['orignalVimputed_05'].count())*100

mean_imputer = mean_imputer.fit(df_number10)
results = mean_imputer.transform(df_number10.values)
imputed_number_df = pd.DataFrame(mean_imputer.transform(results), columns = df_number10.columns)
results.round()
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_10'] = imputed_number_df['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_10'], axis = 0)
percent_change_10 = (imputed_number_df['orignalVimputed_10'].sum() / imputed_number_df['orignalVimputed_10'].count())*100


print("The perecent change of the orignal vs imputed value for 1% missing data is: {}".format(percent_change_1))
print("The perecent change of the orignal vs imputed value for 5% missing data is: {}".format(percent_change_5))
print("The perecent change of the orignal vs imputed value for 10% missing data is: {}".format(percent_change_10))

"""## Using Median for Imputation"""

### Specifying the imputer. ###
mean_imputer = SimpleImputer(missing_values=np.nan, strategy='median')

### FOR the 1 % imputed values using Median method ###
mean_imputer = mean_imputer.fit(df_number1)
results = mean_imputer.transform(df_number1.values)
imputed_number_df = pd.DataFrame(mean_imputer.transform(results), columns = df_number1.columns)
print(imputed_number_df.isnull().sum())
results.round()
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_01'] = imputed_number_df['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_01'], axis = 0)
percent_change_1 = (imputed_number_df['orignalVimputed_01'].sum() / imputed_number_df['orignalVimputed_01'].count())*100

mean_imputer = mean_imputer.fit(df_number5)
results = mean_imputer.transform(df_number5.values)
imputed_number_df = pd.DataFrame(mean_imputer.transform(results), columns = df_number5.columns)
results.round()
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_05'] = imputed_number_df['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_05'], axis = 0)
percent_change_5 = (imputed_number_df['orignalVimputed_05'].sum() / imputed_number_df['orignalVimputed_05'].count())*100

mean_imputer = mean_imputer.fit(df_number10)
results = mean_imputer.transform(df_number10.values)
imputed_number_df = pd.DataFrame(mean_imputer.transform(results), columns = df_number10.columns)
results.round()
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_10'] = imputed_number_df['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_10'], axis = 0)
percent_change_10 = (imputed_number_df['orignalVimputed_10'].sum() / imputed_number_df['orignalVimputed_10'].count())*100


print("The perecent change of the orignal vs imputed value for 1% missing data is: {}".format(percent_change_1))
print("The perecent change of the orignal vs imputed value for 5% missing data is: {}".format(percent_change_5))
print("The perecent change of the orignal vs imputed value for 10% missing data is: {}".format(percent_change_10))

"""## Using KNN"""

from sklearn.metrics import accuracy_score
imputer = KNNImputer(n_neighbors=7)
### For the 1% imputation ###
imputer.fit_transform(df_number1)
imputed_number_df = pd.DataFrame(imputer.fit_transform(df_number1), columns = df_number1.columns)
print(imputed_number_df.isnull().sum())
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_01'] = data_main['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_01'], axis = 0)
percent_change_1 = (imputed_number_df['orignalVimputed_01'].sum() / imputed_number_df['orignalVimputed_01'].count())*100
print("The perecent change of the orignal vs imputed value of 1% values is: {}".format(percent_change_1))

### For the 5% imputation ###
imputer.fit_transform(df_number5)
imputed_number_df = pd.DataFrame(imputer.fit_transform(df_number5), columns = df_number5.columns)
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_05'] = data_main['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_05'], axis = 0)
percent_change_5 = (imputed_number_df['orignalVimputed_05'].sum() / imputed_number_df['orignalVimputed_05'].count())*100
print("The perecent change of the orignal vs imputed value of 5% values is: {}".format(percent_change_5))

### For the 10% imputation ###
imputer.fit_transform(df_number10)
imputed_number_df = pd.DataFrame(imputer.fit_transform(df_number10), columns = df_number10.columns)
imputed_number_df['TotalWorkingYears'] = df_orignal
imputed_number_df['orignalVimputed_10'] = data_main['TotalWorkingYears'].sub(imputed_number_df['TotalWorkingYears_10'], axis = 0)
percent_change_10 = (imputed_number_df['orignalVimputed_10'].sum() / imputed_number_df['orignalVimputed_10'].count())*100
print("The perecent change of the orignal vs imputed value of 10% values is: {}".format(percent_change_10))

#dataFrame_knn_imputation = imputed_number_df[['orignalVimputed_01', 'orignalVimputed_05', 'orignalVimputed_10']]

"""**Answer:** From above we can see that mean has the highest percentage change. We can also see that mean and KNN are predicting values that are higher than the orignal value for 1% and 5% imputed data. For 10% KNN seems to predict values that are a bit lower than the orignal value most of the time. As for median, it seems to be predicting mostly lower than the orignal value. 

|Method| Percentage Imputed| Percentage change|
|------|:-----------------|:------------------|
|Mean | 1%|-0.04|
| | 5%|-3.03|
| | 10%|-10.02|
|Median| 1%| 0.84|
| | 5%|6.47|
| | 10%|0.07|
|KNN| 1%|-0.76|
| | 5%|-0.56|
| | 10%|5.17|

## **MIT License**
Copyright (c) 2023 Tarush Ghanshyam Singh

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

##References 

- https://www.analyticsvidhya.com/blog/2021/09/q-q-plot-ensure-your-ml-model-is-based-on-the-right-distributions/

- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

- https://scikit-learn.org/stable/modules/impute.html

- https://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html#:~:text=Normally%20distributed%20data,-Below%20is%20an&text=The%20normal%20distribution%20is%20symmetric,deviate%20from%20the%20straight%20line).
"""